{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da725b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFECV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, classification_report\n",
    "\n",
    "rng = np.random.RandomState(0)  # initialize random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248562a8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Load the wine dataset: n_samples=178, n_features=13.\n",
    "data, y = load_wine(return_X_y=True, as_frame=True)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46afd63f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Select best features based on mutual information.\n",
    "select_features = SelectKBest(score_func=mutual_info_classif, k=11).fit(data, y)\n",
    "\n",
    "# Plot MI scores\n",
    "fig, ax = plt.subplots(2, 1, figsize=(6, 10), dpi=100)\n",
    "ax[0].bar(np.arange(data.columns.shape[0]), select_features.scores_)\n",
    "ax[0].set_xticks(np.arange(data.shape[1]))\n",
    "ax[0].set(title='Mutual Information scores for features',\n",
    "          xlabel='Feature #', ylabel='MI')\n",
    "\n",
    "# Arbitrary choice: eliminate 2 features with the lowest MI scores.\n",
    "print(\"#: FEATURE NAME\")\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(f'{i}: {col}')\n",
    "print('\\nCan eliminate two features with lowest MI score: ',\n",
    "      data.columns[2], ', ', data.columns[7], '.', sep='')\n",
    "\n",
    "del i, col\n",
    "\n",
    "# Get new dataset (convert to dataframe) with reduced number of features\n",
    "# X = pd.DataFrame(select_features.transform(data), columns=data.columns.delete([2, 7]))\n",
    "\n",
    "# Try recursive feature elimination (with cross-validation) using SVM with linear kernel.\n",
    "clf = SVC(kernel='linear')\n",
    "rfecv = RFECV(clf, step=1, min_features_to_select=1, cv=5, scoring='accuracy')\n",
    "rfecv.fit(data, y)\n",
    "print(f\"\\nOptimal number of features using RFECV: {rfecv.n_features_}\")\n",
    "\n",
    "# Plot number of features vs. cross-validation scores\n",
    "ax[1].plot(np.arange(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, '.-r')\n",
    "ax[1].set(title='Recursive feature elimination using SVM with linear kernel',\n",
    "          xlabel=\"Number of features selected\", ylabel=\"Cross validation score (accuracy)\")\n",
    "ax[1].set_xticks(np.arange(rfecv.grid_scores_.shape[0] + 1))\n",
    "\n",
    "fig.savefig('featureselection.png')\n",
    "# RFE result: keep all features.\n",
    "# Same result when two features with low MI were already eliminated.\n",
    "print('\\nKeeping all 13 features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data infor train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, random_state=rng,\n",
    "                                                    test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506acfc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Try different estimators\n",
    "estimators = [GaussianNB(),\n",
    "              RidgeClassifierCV(alphas=np.logspace(-3, 1, num=10)),\n",
    "              SVC(kernel='linear'),\n",
    "              RandomForestClassifier(random_state=rng)]\n",
    "\n",
    "models = dict()\n",
    "\n",
    "for estimator in estimators:\n",
    "    estimator_name = str(estimator)[:str(estimator).index('(')]\n",
    "\n",
    "    # Make a pipeline\n",
    "    pipe = make_pipeline(StandardScaler(), estimator)\n",
    "    # print(pipe.get_params())\n",
    "\n",
    "    if 'GaussianNB' in estimator_name:\n",
    "        print(\"\\nEstimator: Gaussian Naive Bayes Classifier.\")\n",
    "        model = pipe.fit(X_train, y_train)\n",
    "    elif 'Ridge' in estimator_name:\n",
    "        print(\"\\nEstimator: Ridge Classifier with cross-validation.\")\n",
    "        model = pipe.fit(X_train, y_train)\n",
    "    elif 'SVC' in estimator_name:\n",
    "        print(\"\\nEstimator: Support Vector Machine Classifier.\")\n",
    "        model = pipe.fit(X_train, y_train)\n",
    "    else:\n",
    "        hyperparams = {\"randomforestclassifier__max_features\": [\"auto\", \"sqrt\"],\n",
    "                       \"randomforestclassifier__max_leaf_nodes\": [None, 2, 3, 5],\n",
    "                       \"randomforestclassifier__max_depth\": [None, 1, 3]}\n",
    "        model = GridSearchCV(pipe, hyperparams, cv=10)\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"\\nEstimator: Random Forest Classifier. \\n\"\n",
    "              \"Best parameters after grid search with cross-validation: \\n\"\n",
    "              f\"{model.best_params_}\\nwith score {model.best_score_}\")\n",
    "        # If model.refit is true (default), the model automatically refits to all of X_train.\n",
    "        print(f\"Automatic refit to full X_train: {model.refit}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)  # Predict classes in test set\n",
    "\n",
    "    # *** Calculate metrics of prediction quality ***\n",
    "    # print('Matthews correlation coefficient=', matthews_corrcoef(y_test, y_pred))\n",
    "    # print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Append model to 'models' dict (requires Python 3.9)\n",
    "    models |= {estimator_name: {'model': model,\n",
    "                                'y_pred': y_pred,\n",
    "                                'matthews': matthews_corrcoef(y_test, y_pred),\n",
    "                                'confusion': confusion_matrix(y_test, y_pred)}\n",
    "               }\n",
    "\n",
    "    # Compute learning curve\n",
    "    lc_sizes, train_scores, cv_scores = learning_curve(pipe, X_train, y_train, cv=5,\n",
    "                                                       train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                       scoring='accuracy')\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    cv_scores_mean = np.mean(cv_scores, axis=1)\n",
    "    cv_scores_std = np.std(cv_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    fig, ax = plt.subplots(figsize=(6, 5), dpi=100)\n",
    "    ax.grid()\n",
    "    ax.plot(lc_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    ax.fill_between(lc_sizes, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std,\n",
    "                    alpha=0.1, color=\"r\")\n",
    "\n",
    "    ax.plot(lc_sizes, cv_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    ax.fill_between(lc_sizes, cv_scores_mean - cv_scores_std,\n",
    "                    cv_scores_mean + cv_scores_std,\n",
    "                    alpha=0.1, color=\"g\")\n",
    "\n",
    "    ax.set(title=f\"Learning curve for {estimator_name}\",\n",
    "           xlabel=\"Training examples\", ylabel='Accuracy Score')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    fig.savefig(f'{estimator_name}_LearningCurve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(models, 'wine_ml.joblib')  # save models for later use\n",
    "# models2 = joblib.load('wine_rf.joblib')  # reload models"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
